{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import sympy as sp\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline, PreTrainedModel, PreTrainedTokenizerFast\n",
    "from torch.optim import Adam\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "LLM_MODEL = \"gemma-2-27b\"  # Placeholder model\n",
    "SEARCH_API_KEY = \"YOUR_SEARCH_API_KEY\"\n",
    "MAX_HOTPOT_STEPS = 5\n",
    "MAX_GSM8K_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get AI response\n",
    "def get_openai_response(prompt, model=\"gpt-4o\", log_file=\"./api_log_swirl.txt\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        msg = response.choices[0].message.content.strip()\n",
    "\n",
    "        if log_file != None:\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(\"PROMPT:\\n\\n\" + prompt + \"\\n\\n\" + \"RESPONSE\\n\\n\" + msg + \"\\n\\n\")\n",
    "\n",
    "        return msg\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
